{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2CNA__TDhtr"
      },
      "source": [
        "##Initial process of making M into file and r_old into file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BqBs0isupVPP"
      },
      "outputs": [],
      "source": [
        "#folder = ''\n",
        "import os\n",
        "import re\n",
        "#articles_list = []\n",
        "M_matrix=open('./data/M.txt','w') #opening new empty M file.\n",
        "#r_old_initial=open('r_old','w')\n",
        "for root, dirs, files in os.walk(\"./data/Dataset-mini\"): #iteraing over directory\n",
        "    for file in files: #iterating per file,because here every file is an article instead of combining all files to a single file thaught to iterate over file by file which is article by article for memory efficiency.\n",
        "        if file.endswith('.txt'): #though all are text files this is just an block to perform cleaning operation.\n",
        "            with open(os.path.join(root, file), 'r') as f: #opening every article\n",
        "                text = f.read() #reading every article matter in string format.\n",
        "                y=text.split('</title') #as source of page is between <title> and </title>, just splitted on </title> which makes entire article matter into 2 space array in which 1st index will have <title> source \n",
        "                #next index will have rest of article matter excluding </title>\n",
        "                z=y[0].split('<title>') #now again spliting first index of list on <title> which will give source in z[1] place.\n",
        "                each_row=() #taking tuple which will stores first element as source, second element is degree and third element is list of destinations.\n",
        "                m=[m1.start() for m1 in re.finditer('\\[\\[', y[1])] # as our destinations are in between [[ ]], first finding indices of [[ and storing them in list m.\n",
        "                n=[n1.start() for n1 in re.finditer('\\]\\]', y[1])] # using re.finditer finding indices of only ]] and storing in list n.\n",
        "                destinations=[] #list of destinations to extract strings between indices in list m and list n.\n",
        "                if((len(m)>0 and len(n)>0) and (len(m)==len(n))): #this is important step as some articles may contain only ]] or may contain only [[ which are not required.\n",
        "                  for i in range(0,len(m)):# after filtering we will proceed as [[ ]] these lists length should be equal.\n",
        "                    if(y[1][m[i]+2:n[i]] not in destinations): #now extracting strings between [[ ]] by substring from main string of matter article, using indices stored in m and n.\n",
        "                      destinations.append(y[1][m[i]+2:n[i]]) # appending destinations extracted in [[ ]].\n",
        "                each_row=(z[1],len(destinations),destinations) # appending in tuple as source, degree (len of list of destinations), destinations list.\n",
        "                if(each_row[1]>0): # eleminating the sources which are having zero degrees.\n",
        "                  M_matrix.write(str(each_row[0]) + '||#' + str(each_row[1]) + '||#' +  str(each_row[2])) # writing into M file.\n",
        "                  M_matrix.write(\"\\n\") # for not concatenating with next line.\n",
        "                  #r_old_initial.write(str(each_row[0]) + ':-' + )\n",
        "M_matrix.close()\n",
        "                  \n",
        "#FINALLY AFTER THIS ITERATION MY M.txt WILL HAVE LINES OF SOURCES IN THE FORMAT 'SOURCE'||#'DEGREE'||#'DESTINATIONS' . I PURPOSEDLY WRITTEN INTO FILE LIKE THIS SO THAT IN ALGORITHM I CAN\n",
        "#DIRECTLY SPLIT ON '||#'.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FogBxqaNY1s1",
        "outputId": "9c95a215-9202-4100-ca64-887a519afe8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Number of Page sources in M without zero degree: 1950\n"
          ]
        }
      ],
      "source": [
        "with open(r\"./data/M.txt\", 'r') as fp:\n",
        "  N = len(fp.readlines()) #getting N value for the formula.\n",
        "  print('Total Number of Page sources in M without zero degree:', N)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "grmaaVP0lLEg"
      },
      "outputs": [],
      "source": [
        "beta=0.8 #given beta value.\n",
        "r_new={} #storing r_new hashmap which is allowed and why hashmap because for specific destination updation must be done right so keys will be graph nodes and values will be page ranks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "H7r0-ObQmcj6"
      },
      "outputs": [],
      "source": [
        "#CREATING INTIAL r_old FILE WITH EVERY SOURCE AND  1/N PAGE RANK VALUE.\n",
        "r_old_initial=open('r_old','w') #intialing r_old empty file for writing inital r old.\n",
        "with open('./data/M.txt') as f:\n",
        "    for line in f:\n",
        "        line_cache=line.split('||#') #from M file for every line, splitting on ||# which will give us three spaced array in which first space is source , second space will be degree and third space will be list of destinations.\n",
        "        r_old_initial.write(str(line_cache[0]) + ':-' + str(1/N)) #for every source writing as source :- 1/N \n",
        "        r_old_initial.write(\"\\n\") # for not concatenating next line.\n",
        "        #r_new[line_cache[0]]=((1-beta)/N)\n",
        "r_old_initial.close()\n",
        "\n",
        "#SO FINALLY r_old FILE WILL HAVE ALL NODES AS NODE :- and 1/N VALUE RESPECTIVELY."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lynhZLZv9Ym7"
      },
      "outputs": [],
      "source": [
        "with open('r_old') as f:\n",
        "  for line in f:           \n",
        "    rank_cache=line.split(':-')   # now from r_old file initializing r_new for every line I will split based on :- to get nodes.\n",
        "    r_new[rank_cache[0]]=((1-beta)/N) #storing in hashmap with respective node as key and (1-beta)/N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP5-BSGlDxlV"
      },
      "source": [
        "##Algorithm implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_QD0JNtfJh3A"
      },
      "outputs": [],
      "source": [
        "from ast import literal_eval #necessary package for extracting destinations in algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "10S4wF0Tx7_i"
      },
      "outputs": [],
      "source": [
        "import time #time module has been exported.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wXgPU_BDDwe2"
      },
      "outputs": [],
      "source": [
        "start_time = time.time() #time of execution starts here.\n",
        "for i in range(1,16): # 15 iterations of entire algorithm.\n",
        "  #sum=0\n",
        "  #here r_new have all sources right, so for every page in first line of algorithm means iterating over hashmap which is only in memory.\n",
        "  for x in r_new: # for every source according to algorithm we should read M file for destinations, degree and read r_old file for respective value;\n",
        "    destinations=[]\n",
        "    degree=0 #intializing all required variables for every source. before opening M file and r_old File\n",
        "    r_old_value=0.000000\n",
        "    with open(r\"./data/M.txt\",'r') as f: #Here M file has been opened.\n",
        "      for line_number,line in enumerate(f): #iterating line by line of file instead of storing all mater for memory efficency.\n",
        "        line_cache=line.split('||#') #every line will be splitted on ||# for every iteration.\n",
        "        if(x==line_cache[0]): # if the page source x in r_new is same as source in M\n",
        "          degree=int(line_cache[1]) # taking degree of that source\n",
        "          destinations=literal_eval(line_cache[2]) #literal_eval is the python package which will extract without changing datastructure format from string. So we get destinations list.\n",
        "          break # when we get destinations and degree for one page breaking the loop of scanning of file for time efficiency.\n",
        "    with open(r\"r_old\",'r') as f: #scanning of r_old file for respective r_old value.\n",
        "      for line_number,line in enumerate(f): #reading as for each line instead of storing entire matter for memory efficiency.\n",
        "        r_old_cache=line.split(':-') #splitting on :-\n",
        "        if(r_old_cache[0]==x): #again if we get any source equals to source in r_old file \n",
        "          m = re.findall(r'\\d+\\.\\d+', r_old_cache[1]) #this will give you digits in value from r_old matching line and from second element after splitting on ':-'\n",
        "          r_old_value=float(m[0]) #taking into float of respective value of matched source.\n",
        "          break #breaking the scan of file after getting respective page rank value for time efficiency.\n",
        "   \n",
        "    for destination in destinations:\n",
        "      if(destination in r_new):\n",
        "        for j in range(1,degree+1):                            #last part of algorithm after getting every page's old rank value, degree, destinations.\n",
        "          r_new[destination]=beta*(r_old_value/degree) +(1-beta)/N #appending new ranks with this formula.\n",
        "\n",
        "    sum=0\n",
        "    for x in r_new:\n",
        "      sum+=r_new[x] #taking sum of all pageranks after updating.\n",
        "    for x in r_new:\n",
        "      r_new[x]+=(1-sum)/N #again updating for all pageranks and this ensures less leaking pages.\n",
        "    r_new1=open('r_old','w')\n",
        "    for y in r_new:\n",
        "      r_new1.write(str(y) + ':-' + str(r_new[y])) # this final step for every iteration in which we overwrite r_new values to r_old in r_old file. . This is exacltly doing r_new to r_old after every iteration.\n",
        "      r_new1.write(\"\\n\") #at the end of each iteration r_new is similary written as same format as r_old.\n",
        "    r_new1.close()\n",
        "\n",
        "end_time=time.time()\n",
        "  #end time will be calculated after each iteration.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-F2NwExyHOo",
        "outputId": "35a38fa6-bae5-413a-aec2-3285160f6744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "execution time is :-154.88620257377625\n"
          ]
        }
      ],
      "source": [
        "print('execution time is :-' + str(end_time-start_time)) #execution time of my implementation in seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T-F76mosDZA"
      },
      "source": [
        "##Validity of my implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TGJtR9CdfWf"
      },
      "source": [
        "Generally all page ranks sum should be 1 ideally, but in our graph there are some nodes which can leak and reduce this rank but very slightly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzvUgeZiYSXD",
        "outputId": "5b033a4f-bc44-4a60-98c6-1039366a1b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0000000000000306\n"
          ]
        }
      ],
      "source": [
        "sum=0;\n",
        "for x in r_new: #summing of all page ranks in stored hashmap\n",
        "  sum+=r_new[x]\n",
        "print(sum) # this sums show that my algorithm implementation is valid as because after 15 iterations it reduced from 1 by very very less significant amount by 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4a0JVJ9vNzb"
      },
      "source": [
        "##Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlvpfRWbvNG8",
        "outputId": "93f4385d-1648-4b49-e79c-ed42ebf7ce91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Philosophy: 1.20958e-05\n",
            "Immunology: 7.5232e-06\n",
            "Weapon: 7.5053e-06\n",
            "Time Cube: 7.0531e-06\n",
            "Wikipedia:Basic English international wordlist: 6.5222e-06\n",
            "Libertarianism: 6.2206e-06\n",
            "Link: 5.2417e-06\n",
            "Plymouth Argyle F.C.: 5.2346e-06\n",
            "Now: 5.0685e-06\n",
            "Wikipedia:About: 5.0473e-06\n",
            "Shape of the universe: 4.9682e-06\n",
            "Game: 4.9122e-06\n",
            "List of fruits: 4.874e-06\n",
            "World War: 4.8672e-06\n",
            "Dissolution of the monasteries: 4.7682e-06\n",
            "Help:Revert a page: 4.6054e-06\n",
            "Clothing: 4.5679e-06\n",
            "Lyon: 4.5468e-06\n",
            "Wikipedia:Simple English GFDL: 4.5246e-06\n",
            "Material: 4.5215e-06\n",
            "Asteroid belt: 4.5181e-06\n",
            "The Young and the Restless: 4.1669e-06\n",
            "Peninsula: 4.1631e-06\n",
            "University: 4.1631e-06\n",
            "Wimbledon F.C.: 4.1551e-06\n",
            "Bankside: 4.1346e-06\n",
            "Eric Idle: 4.1311e-06\n",
            "List of lakes: 4.0844e-06\n",
            "Homonym: 4.0817e-06\n",
            "Natural gas: 4.0786e-06\n",
            "Potato: 4.0479e-06\n",
            "Bicycle: 4.0225e-06\n",
            "Tree: 4.0225e-06\n",
            "Spirituality: 4.022e-06\n",
            "Catharism: 3.9219e-06\n",
            "Weather: 3.8432e-06\n",
            "Developmental biology: 3.7973e-06\n",
            "Taxonomy: 3.7973e-06\n",
            "Cytology: 3.7973e-06\n",
            "Flame (disambiguation): 3.776e-06\n",
            "Global: 3.7376e-06\n",
            "Readability: 3.7265e-06\n",
            "Gross domestic product: 3.6672e-06\n",
            "Colonization: 3.6356e-06\n",
            "Beaker: 3.6027e-06\n",
            "Italians: 3.5885e-06\n",
            "Coffee: 3.5837e-06\n",
            "Freedom: 3.5837e-06\n",
            "Leg: 3.5768e-06\n",
            "List of religions: 3.565e-06\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict #package for arranging values based on order.\n",
        "descending_page_ranks = OrderedDict(sorted(r_new.items(), key=lambda x: x[1], reverse=True)) #from r_newsorting based on decending page ranks.\n",
        "output=[] # output list for display.\n",
        "for x in descending_page_ranks:\n",
        "  output.append((x,descending_page_ranks[x]/1000)) # here every page value is very very less which can have something as 3.something *e^-5 something. but printing as string format is allowed only taking number not e^-5 thing ,\n",
        "  #since all ranks sum must be 0.999755339588826 from above to not look awkward divided by 1000 but I checked every value of the ranks and observed to be *e^-5 something which justifies my validity algorithm.\n",
        "for i in range(0,50): # printing top 25 pages with their ranks.\n",
        "  print(str(output[i][0]) + \":\" , round(output[i][1], 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calorimeter: -3.216e-07\n",
            "Sculpting: -3.216e-07\n",
            "Webzine: -3.216e-07\n",
            "Power structure: -3.216e-07\n",
            "Fishing net: -3.216e-07\n",
            "List of elements by name: -3.216e-07\n",
            "Degrees: -3.216e-07\n",
            "Open Content: -3.216e-07\n",
            "Server log: -3.216e-07\n",
            "Mancala: -3.216e-07\n",
            "GURPS: -3.216e-07\n",
            "Political: -3.216e-07\n",
            "CGT: -3.216e-07\n",
            "Mathematical: -3.216e-07\n",
            "Sport utility vehicle: -3.216e-07\n",
            "List of diseases: -3.216e-07\n",
            "MediaWiki:Blocklogtext: -3.216e-07\n",
            "Non-Profit: -3.216e-07\n",
            "Feet: -3.216e-07\n",
            "Speedword: -3.216e-07\n",
            "American Units Of Measurement: -3.216e-07\n",
            "Immigrant: -3.216e-07\n",
            "ISO 19011: -3.216e-07\n",
            "Athens, Greece: -3.216e-07\n",
            "Phase 3: -3.216e-07\n",
            "Models of nature: -3.216e-07\n",
            "Capital: -3.216e-07\n",
            "Graph Theory: -3.216e-07\n",
            "Steal: -3.216e-07\n",
            "List of supermarkets: -3.216e-07\n",
            "MediaWiki:Blockipsuccesstext: -3.216e-07\n",
            "Boot device: -3.216e-07\n",
            "A Brief History of Time: -3.216e-07\n",
            "Phoebe (moon): -3.216e-07\n",
            "Fawlty Towers: -3.216e-07\n",
            "List of vegetables: -3.216e-07\n",
            "Sacred: -3.216e-07\n",
            "Flavor: -3.216e-07\n",
            "Sin: -3.216e-07\n",
            "Herpes zoster: -3.216e-07\n",
            "List Of Common Elements: -3.216e-07\n",
            "Flexsys Cefn Druids F.C.: -3.216e-07\n",
            "Marquis de Sade: -3.216e-07\n",
            "Autonomous Communities Of Spain: -3.216e-07\n",
            "Miracle: -3.216e-07\n",
            "Study skills: -3.216e-07\n",
            "Orange (color): -3.216e-07\n",
            "Template:Please do not change this line: -3.216e-07\n",
            "Conceptual metaphor: -3.216e-07\n",
            "MediaWiki:Autoblocker: -3.216e-07\n"
          ]
        }
      ],
      "source": [
        "for i in range(0,50): # printing top 25 pages with their ranks.\n",
        "  print(str(output[len(output)-1-i][0]) + \":\" , round(output[len(output)-1-i][1], 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
